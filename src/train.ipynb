{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1 - Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-951ca6f26f87>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import array\n",
    "from PIL import Image\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, time, os, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import re\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense, BatchNormalization\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2 - Data loading and Preprocessing\n",
    "\n",
    "Define image and caption path and check how many total images are present in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total Images in Dataset = 8091\n"
     ]
    }
   ],
   "source": [
    "image_path = \"./Flickr_Data/Flickr_Data/Images\"\n",
    "dir_Flickr_text = \"./Flickr_Data/Flickr_Data/Flickr_TextData/Flickr8k.token.txt\"\n",
    "jpgs = os.listdir(image_path)\n",
    "\n",
    "print(\"Total Images in Dataset = {}\".format(len(jpgs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe to store the image id and captions for ease of use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-d1209cf97a19>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m    \u001b[0mdatatxt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatatxt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"filename\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"index\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"caption\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'index'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'filename'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'caption'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'2258277193_586949ec62.jpg.1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "file = open(dir_Flickr_text,'r')\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "datatxt = []\n",
    "for line in text.split('\\n'):\n",
    "   col = line.split('\\t')\n",
    "   if len(col) == 1:\n",
    "       continue\n",
    "   w = col[0].split(\"#\")\n",
    "   datatxt.append(w + [col[1].lower()])\n",
    "\n",
    "data = pd.DataFrame(datatxt,columns=[\"filename\",\"index\",\"caption\"])\n",
    "data = data.reindex(columns =['index','filename','caption'])\n",
    "data = data[data.filename != '2258277193_586949ec62.jpg.1']\n",
    "uni_filenames = np.unique(data.filename.values)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize a few images and their 5 captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npic = 5\n",
    "npix = 224\n",
    "target_size = (npix,npix,3)\n",
    "count = 1\n",
    "\n",
    "fig = plt.figure(figsize=(10,20))\n",
    "for jpgfnm in uni_filenames[10:14]:\n",
    "   filename = image_path + '/' + jpgfnm\n",
    "   captions = list(data[\"caption\"].loc[data[\"filename\"]==jpgfnm].values)\n",
    "   image_load = load_img(filename, target_size=target_size)\n",
    "   ax = fig.add_subplot(npic,2,count,xticks=[],yticks=[])\n",
    "   ax.imshow(image_load)\n",
    "   count += 1\n",
    "\n",
    "   ax = fig.add_subplot(npic,2,count)\n",
    "   plt.axis('off')\n",
    "   ax.plot()\n",
    "   ax.set_xlim(0,1)\n",
    "   ax.set_ylim(0,len(captions))\n",
    "   for i, caption in enumerate(captions):\n",
    "       ax.text(0,i,caption,fontsize=20)\n",
    "   count += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See current vocab size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = []\n",
    "for txt in data.caption.values:\n",
    "   vocabulary.extend(txt.split())\n",
    "print('Vocabulary Size: %d' % len(set(vocabulary)))"
   ]
  },
  {
   "source": [
    "Perform text cleaning such as removing punctuation, single characters, and numeric values"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text_original):\n",
    "   text_no_punctuation = text_original.translate(string.punctuation)\n",
    "   return(text_no_punctuation)\n",
    "\n",
    "def remove_single_character(text):\n",
    "   text_len_more_than1 = \"\"\n",
    "   for word in text.split():\n",
    "       if len(word) > 1:\n",
    "           text_len_more_than1 += \" \" + word\n",
    "   return(text_len_more_than1)\n",
    "\n",
    "def remove_numeric(text):\n",
    "   text_no_numeric = \"\"\n",
    "   for word in text.split():\n",
    "       isalpha = word.isalpha()\n",
    "       if isalpha:\n",
    "           text_no_numeric += \" \" + word\n",
    "   return(text_no_numeric)\n",
    "\n",
    "def text_clean(text_original):\n",
    "   text = remove_punctuation(text_original)\n",
    "   text = remove_single_character(text)\n",
    "   text = remove_numeric(text)\n",
    "   return(text)\n",
    "\n",
    "for i, caption in enumerate(data.caption.values):\n",
    "   newcaption = text_clean(caption)\n",
    "   data[\"caption\"].iloc[i] = newcaption"
   ]
  },
  {
   "source": [
    "Size of vocabulary after cleaning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_vocabulary = []\n",
    "for txt in data.caption.values:\n",
    "   clean_vocabulary.extend(txt.split())\n",
    "print('Clean Vocabulary Size: %d' % len(set(clean_vocabulary)))"
   ]
  },
  {
   "source": [
    "Save all the captions and image paths in two lists so that we can load the images at once using the path set. We also add ‘< start >’ and ‘< end >’ tags to every caption so that the model understands the starting and end of each caption.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"./Flickr_Data/Flickr_Data/\"\n",
    "all_captions = []\n",
    "for caption  in data[\"caption\"].astype(str):\n",
    "   caption = '<start> ' + caption+ ' <end>'\n",
    "   all_captions.append(caption)\n",
    "\n",
    "all_captions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_img_name_vector = []\n",
    "for annot in data[\"filename\"]:\n",
    "   full_image_path = PATH + annot\n",
    "   all_img_name_vector.append(full_image_path)\n",
    "\n",
    "all_img_name_vector[:10]"
   ]
  },
  {
   "source": [
    "Now there are 40455 image paths and captions."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"len(all_img_name_vector) : {len(all_img_name_vector)}\")\n",
    "print(f\"len(all_captions) : {len(all_captions)}\")"
   ]
  },
  {
   "source": [
    "Will take only 40000 of each so that we can select batch size properly, i.e. 625 batches if batch size = 64. To do this, need to define a function to limit the dataset to 40000 images and captions."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_limiter(num,total_captions,all_img_name_vector):\n",
    "   train_captions, img_name_vector = shuffle(total_captions,all_img_name_vector,random_state=1)\n",
    "   train_captions = train_captions[:num]\n",
    "   img_name_vector = img_name_vector[:num]\n",
    "   return train_captions,img_name_vector\n",
    "\n",
    "train_captions,img_name_vector = data_limiter(40000,total_captions,all_img_name_vector)"
   ]
  },
  {
   "source": [
    "Step 3 - Model Definition\n",
    "\n",
    "Define the image feature extraction model using InceptionV3. Must remember that to classify the images here, only need to extract an image vector for our images. Hence, remove the softmax layer from the model. Must preprocess all the images to the same size, i.e, 299×299 before feeding them into the model, and the shape of the output of this layer is 8x8x2048."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "   img = tf.io.read_file(image_path)\n",
    "   img = tf.image.decode_jpeg(img, channels=3)\n",
    "   img = tf.image.resize(img, (299, 299))\n",
    "   img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "   return img, image_path\n",
    "\n",
    "image_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n",
    "new_input = image_model.input\n",
    "hidden_layer = image_model.layers[-1].output\n",
    "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
   ]
  },
  {
   "source": [
    "Map each image name to the function to load the image. Pre-process each image with InceptionV3 and cache the output to disk and image features are reshaped to 64×2048."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_train = sorted(set(img_name_vector))\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
    "image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(64)"
   ]
  },
  {
   "source": [
    "Extract the features and store them in the respective .npy files and then pass those features through the encoder.NPY files. Store all the information required to reconstruct an array on any computer, which includes dtype and shape information."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, path in tqdm(image_dataset):\n",
    "   batch_features = image_features_extract_model(img)\n",
    "   batch_features = tf.reshape(batch_features,\n",
    "                              (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "\n",
    " for bf, p in zip(batch_features, path):\n",
    "   path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "   np.save(path_of_feature, bf.numpy())"
   ]
  },
  {
   "source": [
    "Tokenize the captions and build a vocabulary of all the unique words in the data. Also, limit the vocabulary size to the top 5000 words to save memory and replace words not in vocabulary with the token < unk >."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 5000\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
    "                                                 oov_token=\"<unk>\",\n",
    "                                                 filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "\n",
    "tokenizer.fit_on_texts(train_captions)\n",
    "train_seqs = tokenizer.texts_to_sequences(train_captions)\n",
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'\n",
    "\n",
    "train_seqs = tokenizer.texts_to_sequences(train_captions)\n",
    "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')"
   ]
  },
  {
   "source": [
    "Create training and validation sets using an 80-20 split."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name_train, img_name_val, cap_train, cap_val = train_test_split(img_name_vector,cap_vector, test_size=0.2, random_state=0)"
   ]
  },
  {
   "source": [
    "Create a tf.data dataset to use for training our model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 1000\n",
    "num_steps = len(img_name_train) // BATCH_SIZE\n",
    "\n",
    "def map_func(img_name, cap):\n",
    "   img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
    "   return img_tensor, cap\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n",
    "dataset = dataset.map(lambda item1, item2: tf.numpy_function(map_func, [item1, item2], [tf.float32, tf.int32]),num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "source": [
    "Step 4 - Positional Encoding\n",
    "\n",
    "The positional encoding uses sine and cosine functions of different frequencies. For every odd index on the input vector, create a vector using the cos function, for every even index, create a vector using the sin function. Then add those vectors to their corresponding input embeddings which successfully gives the network information on the position of each vector."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "   angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "   return pos * angle_rates\n",
    "\n",
    "def positional_encoding_1d(position, d_model):\n",
    "   angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                           np.arange(d_model)[np.newaxis, :],\n",
    "                           d_model)\n",
    "\n",
    "   angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "   angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "   pos_encoding = angle_rads[np.newaxis, ...]\n",
    "   return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "def positional_encoding_2d(row,col,d_model):\n",
    "   assert d_model % 2 == 0\n",
    "   row_pos = np.repeat(np.arange(row),col)[:,np.newaxis]\n",
    "   col_pos = np.repeat(np.expand_dims(np.arange(col),0),row,axis=0).reshape(-1,1)\n",
    "\n",
    "   angle_rads_row = get_angles(row_pos,np.arange(d_model//2)[np.newaxis,:],d_model//2)\n",
    "   angle_rads_col = get_angles(col_pos,np.arange(d_model//2)[np.newaxis,:],d_model//2)\n",
    "\n",
    "   angle_rads_row[:, 0::2] = np.sin(angle_rads_row[:, 0::2])\n",
    "   angle_rads_row[:, 1::2] = np.cos(angle_rads_row[:, 1::2])\n",
    "   angle_rads_col[:, 0::2] = np.sin(angle_rads_col[:, 0::2])\n",
    "   angle_rads_col[:, 1::2] = np.cos(angle_rads_col[:, 1::2])\n",
    "   pos_encoding = np.concatenate([angle_rads_row,angle_rads_col],axis=1)[np.newaxis, ...]\n",
    "   return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "source": [
    "Step 5 - Multi-Head Attention\n",
    "\n",
    "Calculate the attention weights. q, k, v must have matching leading dimensions. k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v. The mask has different shapes depending on its type (padding or look ahead) but it must be broadcastable for addition."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "def create_padding_mask(seq):\n",
    "   seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "   return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "   mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "   return mask  # (seq_len, seq_len)\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "   matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "   dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "   scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    ".\n",
    "   if mask is not None:\n",
    "      scaled_attention_logits += (mask * -1e9) \n",
    "\n",
    "   attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1) \n",
    "   output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "   return output, attention_weights\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "   def __init__(self, d_model, num_heads):\n",
    "      super(MultiHeadAttention, self).__init__()\n",
    "      self.num_heads = num_heads\n",
    "      self.d_model = d_model\n",
    "      assert d_model % self.num_heads == 0\n",
    "      self.depth = d_model // self.num_heads\n",
    "      self.wq = tf.keras.layers.Dense(d_model)\n",
    "      self.wk = tf.keras.layers.Dense(d_model)\n",
    "      self.wv = tf.keras.layers.Dense(d_model)\n",
    "      self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "   def split_heads(self, x, batch_size):\n",
    "      x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "      return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "   def call(self, v, k, q, mask=None):\n",
    "      batch_size = tf.shape(q)[0]\n",
    "      q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "      k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "      v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "      q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "      k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "      v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "      scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "      scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q,      num_heads, depth)\n",
    "\n",
    "      concat_attention = tf.reshape(scaled_attention,\n",
    "                                 (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "      output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "      return output, attention_weights\n",
    "\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "   return tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "                tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)])\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "Step 6 - Encoder-Decoder Layer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "   def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "      super(EncoderLayer, self).__init__()\n",
    "      self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "      self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "      self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "      self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "      self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "      self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "\n",
    "   def call(self, x, training, mask=None):\n",
    "      attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "      attn_output = self.dropout1(attn_output, training=training)\n",
    "      out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "      ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "      ffn_output = self.dropout2(ffn_output, training=training)\n",
    "      out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "      return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "   def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "      super(DecoderLayer, self).__init__()\n",
    "      self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "      self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "      self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "      self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "      self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "      self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "      self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "      self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "      self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "   def call(self, x, enc_output, training,look_ahead_mask=None, padding_mask=None):\n",
    "      attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "      attn1 = self.dropout1(attn1, training=training)\n",
    "      out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "      attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask) \n",
    "      attn2 = self.dropout2(attn2, training=training)\n",
    "      out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "      ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "      ffn_output = self.dropout3(ffn_output, training=training)\n",
    "      out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "      return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "   def __init__(self, num_layers, d_model, num_heads, dff, row_size,col_size,rate=0.1):\n",
    "      super(Encoder, self).__init__()\n",
    "      self.d_model = d_model\n",
    "      self.num_layers = num_layers\n",
    "\n",
    "      self.embedding = tf.keras.layers.Dense(self.d_model,activation='relu')\n",
    "      self.pos_encoding = positional_encoding_2d(row_size,col_size,self.d_model)\n",
    "\n",
    "      self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "      self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "   def call(self, x, training, mask=None):\n",
    "      seq_len = tf.shape(x)[1]\n",
    "      x = self.embedding(x)  # (batch_size, input_seq_len(H*W), d_model)\n",
    "      x += self.pos_encoding[:, :seq_len, :]\n",
    "      x = self.dropout(x, training=training)\n",
    "\n",
    "      for i in range(self.num_layers):\n",
    "         x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "      return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "   def __init__(self, num_layers,d_model,num_heads,dff, target_vocab_size, maximum_position_encoding,   rate=0.1):\n",
    "      super(Decoder, self).__init__()\n",
    "      self.d_model = d_model\n",
    "      self.num_layers = num_layers\n",
    "\n",
    "      self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "      self.pos_encoding = positional_encoding_1d(maximum_position_encoding, d_model)\n",
    "\n",
    "      self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)\n",
    "                         for _ in range(num_layers)]\n",
    "      self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "   def call(self, x, enc_output, training,look_ahead_mask=None, padding_mask=None):\n",
    "      seq_len = tf.shape(x)[1]\n",
    "      attention_weights = {}\n",
    "\n",
    "      x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "      x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "      x += self.pos_encoding[:, :seq_len, :]\n",
    "      x = self.dropout(x, training=training)\n",
    "\n",
    "      for i in range(self.num_layers):\n",
    "         x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                            look_ahead_mask, padding_mask)\n",
    "         \n",
    "         attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "         attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "\n",
    "      return x, attention_weights"
   ]
  },
  {
   "source": [
    "Step 7 - Transformer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "   def __init__(self, num_layers, d_model, num_heads, dff,row_size,col_size,\n",
    "              target_vocab_size,max_pos_encoding, rate=0.1):\n",
    "      super(Transformer, self).__init__()\n",
    "      self.encoder = Encoder(num_layers, d_model, num_heads, dff,row_size,col_size, rate)\n",
    "      self.decoder = Decoder(num_layers, d_model, num_heads, dff,\n",
    "                          target_vocab_size,max_pos_encoding, rate)\n",
    "      self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "   def call(self, inp, tar, training,look_ahead_mask=None,dec_padding_mask=None,enc_padding_mask=None   ):\n",
    "      enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model      )\n",
    "      dec_output, attention_weights = self.decoder(\n",
    "      tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "      final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "      return final_output, attention_weights"
   ]
  },
  {
   "source": [
    "Step 8 - Model Hyperparameters\n",
    "\n",
    "Define the parameters for training."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layer = 4\n",
    "d_model = 512\n",
    "dff = 2048\n",
    "num_heads = 8\n",
    "row_size = 8\n",
    "col_size = 8\n",
    "target_vocab_size = top_k + 1\n",
    "dropout_rate = 0.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "   def __init__(self, d_model, warmup_steps=4000):\n",
    "      super(CustomSchedule, self).__init__()\n",
    "      self.d_model = d_model\n",
    "      self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "      self.warmup_steps = warmup_steps\n",
    "\n",
    "   def __call__(self, step):\n",
    "      arg1 = tf.math.rsqrt(step)\n",
    "      arg2 = step * (self.warmup_steps ** -1.5)\n",
    "      return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                    epsilon=1e-9)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "   mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "   loss_ = loss_object(real, pred)\n",
    "   mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "   loss_ *= mask\n",
    "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "transformer = Transformer(\n",
    "    num_layer,\n",
    "    d_model,\n",
    "    num_heads,\n",
    "    dff,\n",
    "    row_size,\n",
    "    col_size,\n",
    "    target_vocab_size, \n",
    "    max_pos_encoding=target_vocab_size,\n",
    "    rate=dropout_rate)"
   ]
  },
  {
   "source": [
    "Step 9 - Model Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks_decoder(tar):\n",
    "   look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "   dec_target_padding_mask = create_padding_mask(tar)\n",
    "   combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "   return combined_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, tar):\n",
    "   tar_inp = tar[:, :-1]\n",
    "   tar_real = tar[:, 1:]\n",
    "   dec_mask = create_masks_decoder(tar_inp)\n",
    "   with tf.GradientTape() as tape:\n",
    "      predictions, _ = transformer(img_tensor, tar_inp,True, dec_mask)\n",
    "      loss = loss_function(tar_real, predictions)\n",
    "\n",
    "   gradients = tape.gradient(loss, transformer.trainable_variables)   \n",
    "   optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "   train_loss(loss)\n",
    "   train_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(30):\n",
    "   start = time.time()\n",
    "   train_loss.reset_states()\n",
    "   train_accuracy.reset_states()\n",
    "   for (batch, (img_tensor, tar)) in enumerate(dataset):\n",
    "      train_step(img_tensor, tar)\n",
    "      if batch % 50 == 0:\n",
    "         print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "         epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "\n",
    "   print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1,\n",
    "                                               train_loss.result(),\n",
    "                                               train_accuracy.result()))\n",
    "   print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "source": [
    "Step 10 - BLEU Evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(image):\n",
    "   temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
    "   img_tensor_val = image_features_extract_model(temp_input)\n",
    "   img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "   start_token = tokenizer.word_index['<start>']\n",
    "   end_token = tokenizer.word_index['<end>']\n",
    "   decoder_input = [start_token]\n",
    "   output = tf.expand_dims(decoder_input, 0) #tokens\n",
    "   result = [] #word list\n",
    "\n",
    "   for i in range(100):\n",
    "      dec_mask = create_masks_decoder(output)\n",
    "      predictions, attention_weights = transformer(img_tensor_val,output,False,dec_mask)\n",
    "      predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "      predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "      if predicted_id == end_token:\n",
    "         return result,tf.squeeze(output, axis=0), attention_weights\n",
    "      result.append(tokenizer.index_word[int(predicted_id)])\n",
    "      output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "   return result,tf.squeeze(output, axis=0), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rid = np.random.randint(0, len(img_name_val))\n",
    "image = img_name_val[rid]\n",
    "real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n",
    "caption,result,attention_weights = evaluate(image)\n",
    "\n",
    "first = real_caption.split(' ', 1)[1]\n",
    "real_caption = first.rsplit(' ', 1)[0]\n",
    "\n",
    "for i in caption:\n",
    "   if i==\"<unk>\":\n",
    "      caption.remove(i)\n",
    "\n",
    "for i in real_caption:\n",
    "   if i==\"<unk>\":\n",
    "      real_caption.remove(i)\n",
    "\n",
    "result_join = ' '.join(caption)\n",
    "result_final = result_join.rsplit(' ', 1)[0]\n",
    "real_appn = []\n",
    "real_appn.append(real_caption.split())\n",
    "reference = real_appn\n",
    "candidate = caption\n",
    "\n",
    "score = sentence_bleu(reference, candidate, weights=(1.0,0,0,0))\n",
    "print(f\"BLEU-1 score: {score*100}\")\n",
    "score = sentence_bleu(reference, candidate, weights=(0.5,0.5,0,0))\n",
    "print(f\"BLEU-2 score: {score*100}\")\n",
    "score = sentence_bleu(reference, candidate, weights=(0.3,0.3,0.3,0))\n",
    "print(f\"BLEU-3 score: {score*100}\")\n",
    "score = sentence_bleu(reference, candidate, weights=(0.25,0.25,0.25,0.25))\n",
    "print(f\"BLEU-4 score: {score*100}\")\n",
    "print ('Real Caption:', real_caption)\n",
    "print ('Predicted Caption:', ' '.join(caption))\n",
    "temp_image = np.array(Image.open(image))\n",
    "plt.imshow(temp_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rid = np.random.randint(0, len(img_name_val))\n",
    "image = img_name_val[rid]\n",
    "real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n",
    "caption,result,attention_weights = evaluate(image)\n",
    "\n",
    "first = real_caption.split(' ', 1)[1]\n",
    "real_caption = first.rsplit(' ', 1)[0]\n",
    "\n",
    "for i in caption:\n",
    "   if i==\"<unk>\":\n",
    "      caption.remove(i)\n",
    "\n",
    "for i in real_caption:\n",
    "   if i==\"<unk>\":\n",
    "      real_caption.remove(i)\n",
    "\n",
    "result_join = ' '.join(caption)\n",
    "result_final = result_join.rsplit(' ', 1)[0]\n",
    "real_appn = []\n",
    "real_appn.append(real_caption.split())\n",
    "reference = real_appn\n",
    "candidate = caption\n",
    "\n",
    "score = sentence_bleu(reference, candidate, weights=(1.0,0,0,0))\n",
    "print(f\"BLEU-1 score: {score*100}\")\n",
    "score = sentence_bleu(reference, candidate, weights=(0.5,0.5,0,0))\n",
    "print(f\"BLEU-2 score: {score*100}\")\n",
    "score = sentence_bleu(reference, candidate, weights=(0.3,0.3,0.3,0))\n",
    "print(f\"BLEU-3 score: {score*100}\")\n",
    "score = sentence_bleu(reference, candidate, weights=(0.25,0.25,0.25,0.25))\n",
    "print(f\"BLEU-4 score: {score*100}\")\n",
    "print ('Real Caption:', real_caption)\n",
    "print ('Predicted Caption:', ' '.join(caption))\n",
    "temp_image = np.array(Image.open(image))\n",
    "plt.imshow(temp_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "2dcab63b871fdc3becb35bef91fbc89dccdc93cc3d2e3ce3dee52bc9950873c4"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}